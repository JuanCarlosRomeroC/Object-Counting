{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from neural_networks.unet import UNet\n",
    "from neural_networks.net_utils import predict_density_maps_and_get_counts\n",
    "from utils.data.data_generator import DataGenerator\n",
    "from utils.evaluation.evaluation import evaluation_results_as_dict\n",
    "from utils.evaluation.evaluation import evaluation_results_as_df\n",
    "from utils.input_output.io import load_images_and_density_maps\n",
    "from utils.input_output.io import read_json, write_json\n",
    "from utils.input_output.io import load_gt_counts\n",
    "from utils.visualization.vis import plot_some_predictions\n",
    "from utils.visualization.vis import plot_gt_vs_pred_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dim': config.IMG_DIM,\n",
    "    'batch_size': 1,\n",
    "    'patches_per_image': 1,\n",
    "    'density_map_multiplication_factor': config.DENSITY_MAP_MULTIPLICATION_FACTOR,\n",
    "    'shuffle': False,\n",
    "    'data_augmentation': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(config.DATASET_PATH, 'train', **params)\n",
    "val_generator = DataGenerator(config.DATASET_PATH, 'val', **params)\n",
    "test_generator = DataGenerator(config.DATASET_PATH, 'test', **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt_counts = load_gt_counts(config.TRAIN_GT_COUNT_PATH)\n",
    "val_gt_counts = load_gt_counts(config.VAL_GT_COUNT_PATH)\n",
    "test_gt_counts = load_gt_counts(config.TEST_GT_COUNT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the checkpoint file that you want to test/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.01-0.016.hdf5', 'model.02-0.011.hdf5', 'model.03-0.009.hdf5', 'model.04-0.009.hdf5', 'model.05-0.008.hdf5', 'model.06-0.007.hdf5', 'model.07-0.007.hdf5', 'model.08-0.007.hdf5', 'model.09-0.007.hdf5', 'model.10-0.006.hdf5', 'model.11-0.007.hdf5', 'model.12-0.006.hdf5', 'model.13-0.006.hdf5']\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filenames = sorted(os.listdir(config.CHECKPOINTS_PATH))\n",
    "print(checkpoint_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected checkpoint_filename: model.06-0.007.hdf5\n",
      "epoch: 06\n",
      "selected checkpoint_filename: model.07-0.007.hdf5\n",
      "epoch: 07\n",
      "selected checkpoint_filename: model.08-0.007.hdf5\n",
      "epoch: 08\n",
      "selected checkpoint_filename: model.09-0.007.hdf5\n",
      "epoch: 09\n",
      "selected checkpoint_filename: model.10-0.006.hdf5\n",
      "epoch: 10\n",
      "selected checkpoint_filename: model.11-0.007.hdf5\n",
      "epoch: 11\n",
      "selected checkpoint_filename: model.12-0.006.hdf5\n",
      "epoch: 12\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_idx in range(5, 12):\n",
    "    selected_checkpoint_filename = checkpoint_filenames[checkpoint_idx]\n",
    "    print(f'selected checkpoint_filename: {selected_checkpoint_filename}')\n",
    "    \n",
    "    epoch = selected_checkpoint_filename.split('.')[1].split('-')[0]\n",
    "    print('epoch:', epoch)\n",
    "    \n",
    "    # Set epoch and val loss\n",
    "    CHECKPOINT_FILENAME = f'{config.CHECKPOINTS_PATH}/{selected_checkpoint_filename}'\n",
    "    QUANTITATIVE_RESULTS_PATH = f'./{config.SUB_EXPERIMENT_NAME}/results/quantitative/epoch_{epoch}'\n",
    "    \n",
    "    !rm -rf $QUANTITATIVE_RESULTS_PATH\n",
    "    os.makedirs(QUANTITATIVE_RESULTS_PATH)\n",
    "    \n",
    "    ## 3. Load the best model\n",
    "    model = UNet(pretrained_weights=CHECKPOINT_FILENAME)\n",
    "\n",
    "    train_pred_counts = predict_density_maps_and_get_counts(model, train_generator,\n",
    "                                                        config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    val_pred_counts = predict_density_maps_and_get_counts(model, val_generator,\n",
    "                                                          config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    test_pred_counts = predict_density_maps_and_get_counts(model, test_generator,\n",
    "                                                           config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    \n",
    "    ## 4. Predict and evaluate\n",
    "    train_results = evaluation_results_as_dict(train_gt_counts, train_pred_counts, 'train')\n",
    "    val_results = evaluation_results_as_dict(val_gt_counts, val_pred_counts, 'val')\n",
    "    test_results = evaluation_results_as_dict(test_gt_counts, test_pred_counts, 'test')\n",
    "\n",
    "    df = evaluation_results_as_df(train_results, val_results, test_results,\n",
    "                                  config.ARCHITECTURE_NAME,\n",
    "                                  config.SUB_EXPERIMENT_NAME,\n",
    "                                  config.DATASET_NAME)\n",
    "\n",
    "    df.to_csv(f'{QUANTITATIVE_RESULTS_PATH}/results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
