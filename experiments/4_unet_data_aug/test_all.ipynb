{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from neural_networks.unet import UNet\n",
    "from neural_networks.net_utils import predict_density_maps_and_get_counts\n",
    "from utils.data.data_generator import DataGenerator\n",
    "from utils.evaluation.evaluation import evaluation_results_as_dict\n",
    "from utils.evaluation.evaluation import evaluation_results_as_df\n",
    "from utils.input_output.io import load_images_and_density_maps\n",
    "from utils.input_output.io import read_json, write_json\n",
    "from utils.input_output.io import load_gt_counts\n",
    "from utils.visualization.vis import plot_some_predictions\n",
    "from utils.visualization.vis import plot_gt_vs_pred_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dim': config.IMG_DIM,\n",
    "    'batch_size': 1,\n",
    "    'patches_per_image': 1,\n",
    "    'density_map_multiplication_factor': config.DENSITY_MAP_MULTIPLICATION_FACTOR,\n",
    "    'shuffle': False,\n",
    "    'data_augmentation': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(config.DATASET_PATH, 'train', **params)\n",
    "val_generator = DataGenerator(config.DATASET_PATH, 'val', **params)\n",
    "test_generator = DataGenerator(config.DATASET_PATH, 'test', **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt_counts = load_gt_counts(config.TRAIN_GT_COUNT_PATH)\n",
    "val_gt_counts = load_gt_counts(config.VAL_GT_COUNT_PATH)\n",
    "test_gt_counts = load_gt_counts(config.TEST_GT_COUNT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the checkpoint file that you want to test/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.01-0.178.hdf5', 'model.02-0.168.hdf5', 'model.03-0.143.hdf5', 'model.04-0.140.hdf5', 'model.05-0.135.hdf5', 'model.06-0.176.hdf5', 'model.07-0.124.hdf5', 'model.08-0.115.hdf5', 'model.09-0.129.hdf5', 'model.10-0.113.hdf5', 'model.11-0.116.hdf5', 'model.12-0.109.hdf5', 'model.13-0.108.hdf5', 'model.14-0.106.hdf5', 'model.15-0.097.hdf5', 'model.16-0.097.hdf5', 'model.17-0.095.hdf5', 'model.18-0.095.hdf5', 'model.19-0.110.hdf5', 'model.20-0.094.hdf5', 'model.21-0.092.hdf5', 'model.22-0.092.hdf5', 'model.23-0.092.hdf5', 'model.24-0.091.hdf5', 'model.25-0.097.hdf5', 'model.26-0.093.hdf5', 'model.27-0.092.hdf5', 'model.28-0.090.hdf5', 'model.29-0.092.hdf5', 'model.30-0.090.hdf5', 'model.31-0.094.hdf5', 'model.32-0.092.hdf5', 'model.33-0.103.hdf5', 'model.34-0.092.hdf5', 'model.35-0.092.hdf5', 'model.36-0.092.hdf5', 'model.37-0.091.hdf5', 'model.38-0.091.hdf5', 'model.39-0.090.hdf5', 'model.40-0.097.hdf5', 'model.41-0.096.hdf5', 'model.42-0.096.hdf5', 'model.43-0.092.hdf5', 'model.44-0.098.hdf5', 'model.45-0.092.hdf5', 'model.46-0.097.hdf5', 'model.47-0.092.hdf5', 'model.48-0.092.hdf5', 'model.49-0.092.hdf5', 'model.50-0.091.hdf5']\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filenames = sorted(os.listdir(config.CHECKPOINTS_PATH))\n",
    "print(checkpoint_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected checkpoint_filename: model.01-0.178.hdf5\n",
      "epoch: 01\n",
      "selected checkpoint_filename: model.03-0.143.hdf5\n",
      "epoch: 03\n",
      "selected checkpoint_filename: model.05-0.135.hdf5\n",
      "epoch: 05\n",
      "selected checkpoint_filename: model.07-0.124.hdf5\n",
      "epoch: 07\n",
      "selected checkpoint_filename: model.09-0.129.hdf5\n",
      "epoch: 09\n",
      "selected checkpoint_filename: model.11-0.116.hdf5\n",
      "epoch: 11\n",
      "selected checkpoint_filename: model.13-0.108.hdf5\n",
      "epoch: 13\n",
      "selected checkpoint_filename: model.15-0.097.hdf5\n",
      "epoch: 15\n",
      "selected checkpoint_filename: model.17-0.095.hdf5\n",
      "epoch: 17\n",
      "selected checkpoint_filename: model.19-0.110.hdf5\n",
      "epoch: 19\n",
      "selected checkpoint_filename: model.21-0.092.hdf5\n",
      "epoch: 21\n",
      "selected checkpoint_filename: model.23-0.092.hdf5\n",
      "epoch: 23\n",
      "selected checkpoint_filename: model.25-0.097.hdf5\n",
      "epoch: 25\n",
      "selected checkpoint_filename: model.27-0.092.hdf5\n",
      "epoch: 27\n",
      "selected checkpoint_filename: model.29-0.092.hdf5\n",
      "epoch: 29\n",
      "selected checkpoint_filename: model.31-0.094.hdf5\n",
      "epoch: 31\n",
      "selected checkpoint_filename: model.33-0.103.hdf5\n",
      "epoch: 33\n",
      "selected checkpoint_filename: model.35-0.092.hdf5\n",
      "epoch: 35\n",
      "selected checkpoint_filename: model.37-0.091.hdf5\n",
      "epoch: 37\n",
      "selected checkpoint_filename: model.39-0.090.hdf5\n",
      "epoch: 39\n",
      "selected checkpoint_filename: model.41-0.096.hdf5\n",
      "epoch: 41\n",
      "selected checkpoint_filename: model.43-0.092.hdf5\n",
      "epoch: 43\n",
      "selected checkpoint_filename: model.45-0.092.hdf5\n",
      "epoch: 45\n",
      "selected checkpoint_filename: model.47-0.092.hdf5\n",
      "epoch: 47\n",
      "selected checkpoint_filename: model.49-0.092.hdf5\n",
      "epoch: 49\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_idx in range(0, 50, 2):\n",
    "    selected_checkpoint_filename = checkpoint_filenames[checkpoint_idx]\n",
    "    print(f'selected checkpoint_filename: {selected_checkpoint_filename}')\n",
    "    \n",
    "    epoch = selected_checkpoint_filename.split('.')[1].split('-')[0]\n",
    "    print('epoch:', epoch)\n",
    "    \n",
    "    # Set epoch and val loss\n",
    "    CHECKPOINT_FILENAME = f'{config.CHECKPOINTS_PATH}/{selected_checkpoint_filename}'\n",
    "    QUANTITATIVE_RESULTS_PATH = f'./{config.SUB_EXPERIMENT_NAME}/results/quantitative/epoch_{epoch}'\n",
    "    \n",
    "    !rm -rf $QUANTITATIVE_RESULTS_PATH\n",
    "    os.makedirs(QUANTITATIVE_RESULTS_PATH)\n",
    "    \n",
    "    ## 3. Load the best model\n",
    "    model = UNet(pretrained_weights=CHECKPOINT_FILENAME)\n",
    "\n",
    "    train_pred_counts = predict_density_maps_and_get_counts(model, train_generator,\n",
    "                                                        config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    val_pred_counts = predict_density_maps_and_get_counts(model, val_generator,\n",
    "                                                          config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    test_pred_counts = predict_density_maps_and_get_counts(model, test_generator,\n",
    "                                                           config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    \n",
    "    ## 4. Predict and evaluate\n",
    "    train_results = evaluation_results_as_dict(train_gt_counts, train_pred_counts, 'train')\n",
    "    val_results = evaluation_results_as_dict(val_gt_counts, val_pred_counts, 'val')\n",
    "    test_results = evaluation_results_as_dict(test_gt_counts, test_pred_counts, 'test')\n",
    "\n",
    "    df = evaluation_results_as_df(train_results, val_results, test_results,\n",
    "                                  config.ARCHITECTURE_NAME,\n",
    "                                  config.SUB_EXPERIMENT_NAME,\n",
    "                                  config.DATASET_NAME)\n",
    "\n",
    "    df.to_csv(f'{QUANTITATIVE_RESULTS_PATH}/results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
