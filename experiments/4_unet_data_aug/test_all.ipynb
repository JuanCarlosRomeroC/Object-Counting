{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from neural_networks.unet import UNet\n",
    "from neural_networks.net_utils import predict_density_maps_and_get_counts\n",
    "from utils.data.data_generator import DataGenerator\n",
    "from utils.evaluation.evaluation import evaluation_results_as_dict\n",
    "from utils.evaluation.evaluation import evaluation_results_as_df\n",
    "from utils.input_output.io import load_images_and_density_maps\n",
    "from utils.input_output.io import read_json, write_json\n",
    "from utils.input_output.io import load_gt_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dim': config.IMG_DIM,\n",
    "    'batch_size': 1,\n",
    "    'patches_per_image': 1,\n",
    "    'density_map_multiplication_factor': config.DENSITY_MAP_MULTIPLICATION_FACTOR,\n",
    "    'shuffle': False,\n",
    "    'data_augmentation': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(config.DATASET_PATH, 'train', **params)\n",
    "val_generator = DataGenerator(config.DATASET_PATH, 'val', **params)\n",
    "test_generator = DataGenerator(config.DATASET_PATH, 'test', **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt_counts = load_gt_counts(config.TRAIN_GT_COUNT_PATH)\n",
    "val_gt_counts = load_gt_counts(config.VAL_GT_COUNT_PATH)\n",
    "test_gt_counts = load_gt_counts(config.TEST_GT_COUNT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the checkpoint file that you want to test/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.01-0.194.hdf5', 'model.02-0.174.hdf5', 'model.03-0.150.hdf5', 'model.04-0.151.hdf5', 'model.05-0.144.hdf5', 'model.06-0.134.hdf5', 'model.07-0.123.hdf5', 'model.08-0.127.hdf5', 'model.09-0.156.hdf5', 'model.10-0.129.hdf5', 'model.11-0.118.hdf5', 'model.12-0.127.hdf5', 'model.13-0.112.hdf5', 'model.14-0.110.hdf5', 'model.15-0.103.hdf5', 'model.16-0.104.hdf5', 'model.17-0.108.hdf5', 'model.18-0.103.hdf5', 'model.19-0.115.hdf5', 'model.20-0.102.hdf5', 'model.21-0.099.hdf5', 'model.22-0.101.hdf5', 'model.23-0.097.hdf5', 'model.24-0.094.hdf5', 'model.25-0.101.hdf5', 'model.26-0.094.hdf5', 'model.27-0.093.hdf5', 'model.28-0.111.hdf5', 'model.29-0.089.hdf5', 'model.30-0.090.hdf5', 'model.31-0.092.hdf5', 'model.32-0.089.hdf5', 'model.33-0.098.hdf5', 'model.34-0.095.hdf5', 'model.35-0.086.hdf5', 'model.36-0.099.hdf5', 'model.37-0.086.hdf5', 'model.38-0.093.hdf5', 'model.39-0.085.hdf5', 'model.40-0.088.hdf5', 'model.41-0.085.hdf5', 'model.42-0.084.hdf5', 'model.43-0.089.hdf5', 'model.44-0.085.hdf5', 'model.45-0.085.hdf5', 'model.46-0.086.hdf5', 'model.47-0.082.hdf5', 'model.48-0.085.hdf5', 'model.49-0.087.hdf5', 'model.50-0.082.hdf5']\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filenames = sorted(os.listdir(config.CHECKPOINTS_PATH))\n",
    "print(checkpoint_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected checkpoint_filename: model.31-0.092.hdf5\n",
      "epoch: 31\n",
      "selected checkpoint_filename: model.32-0.089.hdf5\n",
      "epoch: 32\n",
      "selected checkpoint_filename: model.33-0.098.hdf5\n",
      "epoch: 33\n",
      "selected checkpoint_filename: model.34-0.095.hdf5\n",
      "epoch: 34\n",
      "selected checkpoint_filename: model.35-0.086.hdf5\n",
      "epoch: 35\n",
      "selected checkpoint_filename: model.36-0.099.hdf5\n",
      "epoch: 36\n",
      "selected checkpoint_filename: model.37-0.086.hdf5\n",
      "epoch: 37\n",
      "selected checkpoint_filename: model.38-0.093.hdf5\n",
      "epoch: 38\n",
      "selected checkpoint_filename: model.39-0.085.hdf5\n",
      "epoch: 39\n",
      "selected checkpoint_filename: model.40-0.088.hdf5\n",
      "epoch: 40\n",
      "selected checkpoint_filename: model.41-0.085.hdf5\n",
      "epoch: 41\n",
      "selected checkpoint_filename: model.42-0.084.hdf5\n",
      "epoch: 42\n",
      "selected checkpoint_filename: model.43-0.089.hdf5\n",
      "epoch: 43\n",
      "selected checkpoint_filename: model.44-0.085.hdf5\n",
      "epoch: 44\n",
      "selected checkpoint_filename: model.45-0.085.hdf5\n",
      "epoch: 45\n",
      "selected checkpoint_filename: model.46-0.086.hdf5\n",
      "epoch: 46\n",
      "selected checkpoint_filename: model.47-0.082.hdf5\n",
      "epoch: 47\n",
      "selected checkpoint_filename: model.48-0.085.hdf5\n",
      "epoch: 48\n",
      "selected checkpoint_filename: model.49-0.087.hdf5\n",
      "epoch: 49\n",
      "selected checkpoint_filename: model.50-0.082.hdf5\n",
      "epoch: 50\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_idx in range(30, 50, 1):\n",
    "    selected_checkpoint_filename = checkpoint_filenames[checkpoint_idx]\n",
    "    print(f'selected checkpoint_filename: {selected_checkpoint_filename}')\n",
    "    \n",
    "    epoch = selected_checkpoint_filename.split('.')[1].split('-')[0]\n",
    "    print('epoch:', epoch)\n",
    "    \n",
    "    # Set epoch and val loss\n",
    "    CHECKPOINT_FILENAME = f'{config.CHECKPOINTS_PATH}/{selected_checkpoint_filename}'\n",
    "    QUANTITATIVE_RESULTS_PATH = f'./{config.SUB_EXPERIMENT_NAME}/results/quantitative/epoch_{epoch}'\n",
    "    \n",
    "    !rm -rf $QUANTITATIVE_RESULTS_PATH\n",
    "    os.makedirs(QUANTITATIVE_RESULTS_PATH)\n",
    "    \n",
    "    ## 3. Load the best model\n",
    "    model = UNet(pretrained_weights=CHECKPOINT_FILENAME)\n",
    "\n",
    "    train_pred_counts = predict_density_maps_and_get_counts(model, train_generator,\n",
    "                                                        config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    val_pred_counts = predict_density_maps_and_get_counts(model, val_generator,\n",
    "                                                          config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    test_pred_counts = predict_density_maps_and_get_counts(model, test_generator,\n",
    "                                                           config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    \n",
    "    ## 4. Predict and evaluate\n",
    "    train_results = evaluation_results_as_dict(train_gt_counts, train_pred_counts, 'train')\n",
    "    val_results = evaluation_results_as_dict(val_gt_counts, val_pred_counts, 'val')\n",
    "    test_results = evaluation_results_as_dict(test_gt_counts, test_pred_counts, 'test')\n",
    "\n",
    "    df = evaluation_results_as_df(train_results, val_results, test_results,\n",
    "                                  config.ARCHITECTURE_NAME,\n",
    "                                  config.SUB_EXPERIMENT_NAME,\n",
    "                                  config.DATASET_NAME)\n",
    "\n",
    "    df.to_csv(f'{QUANTITATIVE_RESULTS_PATH}/results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
