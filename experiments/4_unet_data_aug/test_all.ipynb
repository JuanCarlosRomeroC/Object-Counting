{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from neural_networks.unet import UNet\n",
    "from neural_networks.net_utils import predict_density_maps_and_get_counts\n",
    "from utils.data.data_generator import DataGenerator\n",
    "from utils.evaluation.evaluation import evaluation_results_as_dict\n",
    "from utils.evaluation.evaluation import evaluation_results_as_df\n",
    "from utils.input_output.io import load_images_and_density_maps\n",
    "from utils.input_output.io import read_json, write_json\n",
    "from utils.input_output.io import load_gt_counts\n",
    "from utils.visualization.vis import plot_some_predictions\n",
    "from utils.visualization.vis import plot_gt_vs_pred_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'dim': config.IMG_DIM,\n",
    "    'batch_size': 1,\n",
    "    'patches_per_image': 1,\n",
    "    'density_map_multiplication_factor': config.DENSITY_MAP_MULTIPLICATION_FACTOR,\n",
    "    'shuffle': False,\n",
    "    'data_augmentation': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(config.DATASET_PATH, 'train', **params)\n",
    "val_generator = DataGenerator(config.DATASET_PATH, 'val', **params)\n",
    "test_generator = DataGenerator(config.DATASET_PATH, 'test', **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt_counts = load_gt_counts(config.TRAIN_GT_COUNT_PATH)\n",
    "val_gt_counts = load_gt_counts(config.VAL_GT_COUNT_PATH)\n",
    "test_gt_counts = load_gt_counts(config.TEST_GT_COUNT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the checkpoint file that you want to test/evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.01-1.062.hdf5', 'model.02-0.987.hdf5', 'model.03-0.906.hdf5', 'model.04-0.862.hdf5', 'model.05-1.185.hdf5', 'model.06-0.993.hdf5', 'model.07-0.719.hdf5', 'model.08-0.814.hdf5', 'model.09-0.826.hdf5', 'model.10-0.680.hdf5', 'model.11-0.738.hdf5', 'model.12-0.701.hdf5', 'model.13-0.679.hdf5', 'model.14-0.609.hdf5', 'model.15-0.686.hdf5', 'model.16-0.660.hdf5', 'model.17-0.599.hdf5', 'model.18-0.574.hdf5', 'model.19-0.592.hdf5', 'model.20-0.713.hdf5', 'model.21-0.558.hdf5', 'model.22-0.667.hdf5', 'model.23-0.552.hdf5', 'model.24-0.608.hdf5', 'model.25-0.526.hdf5', 'model.26-0.514.hdf5', 'model.27-0.538.hdf5', 'model.28-0.507.hdf5', 'model.29-0.570.hdf5', 'model.30-0.492.hdf5', 'model.31-0.499.hdf5', 'model.32-0.502.hdf5', 'model.33-0.543.hdf5', 'model.34-0.479.hdf5', 'model.35-0.586.hdf5', 'model.36-0.493.hdf5', 'model.37-0.619.hdf5', 'model.38-0.548.hdf5', 'model.39-0.479.hdf5', 'model.40-0.477.hdf5', 'model.41-0.476.hdf5', 'model.42-0.478.hdf5', 'model.43-0.478.hdf5', 'model.44-0.487.hdf5', 'model.45-0.486.hdf5', 'model.46-0.476.hdf5', 'model.47-0.488.hdf5', 'model.48-0.486.hdf5', 'model.49-0.484.hdf5', 'model.50-0.477.hdf5']\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filenames = sorted(os.listdir(config.CHECKPOINTS_PATH))\n",
    "print(checkpoint_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected checkpoint_filename: model.02-0.987.hdf5\n",
      "epoch: 02\n",
      "selected checkpoint_filename: model.04-0.862.hdf5\n",
      "epoch: 04\n",
      "selected checkpoint_filename: model.06-0.993.hdf5\n",
      "epoch: 06\n",
      "selected checkpoint_filename: model.08-0.814.hdf5\n",
      "epoch: 08\n",
      "selected checkpoint_filename: model.10-0.680.hdf5\n",
      "epoch: 10\n",
      "selected checkpoint_filename: model.12-0.701.hdf5\n",
      "epoch: 12\n",
      "selected checkpoint_filename: model.14-0.609.hdf5\n",
      "epoch: 14\n",
      "selected checkpoint_filename: model.16-0.660.hdf5\n",
      "epoch: 16\n",
      "selected checkpoint_filename: model.18-0.574.hdf5\n",
      "epoch: 18\n",
      "selected checkpoint_filename: model.20-0.713.hdf5\n",
      "epoch: 20\n",
      "selected checkpoint_filename: model.22-0.667.hdf5\n",
      "epoch: 22\n",
      "selected checkpoint_filename: model.24-0.608.hdf5\n",
      "epoch: 24\n",
      "selected checkpoint_filename: model.26-0.514.hdf5\n",
      "epoch: 26\n",
      "selected checkpoint_filename: model.28-0.507.hdf5\n",
      "epoch: 28\n",
      "selected checkpoint_filename: model.30-0.492.hdf5\n",
      "epoch: 30\n",
      "selected checkpoint_filename: model.32-0.502.hdf5\n",
      "epoch: 32\n",
      "selected checkpoint_filename: model.34-0.479.hdf5\n",
      "epoch: 34\n",
      "selected checkpoint_filename: model.36-0.493.hdf5\n",
      "epoch: 36\n",
      "selected checkpoint_filename: model.38-0.548.hdf5\n",
      "epoch: 38\n",
      "selected checkpoint_filename: model.40-0.477.hdf5\n",
      "epoch: 40\n",
      "selected checkpoint_filename: model.42-0.478.hdf5\n",
      "epoch: 42\n",
      "selected checkpoint_filename: model.44-0.487.hdf5\n",
      "epoch: 44\n",
      "selected checkpoint_filename: model.46-0.476.hdf5\n",
      "epoch: 46\n",
      "selected checkpoint_filename: model.48-0.486.hdf5\n",
      "epoch: 48\n",
      "selected checkpoint_filename: model.50-0.477.hdf5\n",
      "epoch: 50\n"
     ]
    }
   ],
   "source": [
    "for checkpoint_idx in range(1, 50, 2):\n",
    "    selected_checkpoint_filename = checkpoint_filenames[checkpoint_idx]\n",
    "    print(f'selected checkpoint_filename: {selected_checkpoint_filename}')\n",
    "    \n",
    "    epoch = selected_checkpoint_filename.split('.')[1].split('-')[0]\n",
    "    print('epoch:', epoch)\n",
    "    \n",
    "    # Set epoch and val loss\n",
    "    CHECKPOINT_FILENAME = f'{config.CHECKPOINTS_PATH}/{selected_checkpoint_filename}'\n",
    "    QUANTITATIVE_RESULTS_PATH = f'./{config.SUB_EXPERIMENT_NAME}/results/quantitative/epoch_{epoch}'\n",
    "    \n",
    "    !rm -rf $QUANTITATIVE_RESULTS_PATH\n",
    "    os.makedirs(QUANTITATIVE_RESULTS_PATH)\n",
    "    \n",
    "    ## 3. Load the best model\n",
    "    model = UNet(pretrained_weights=CHECKPOINT_FILENAME)\n",
    "\n",
    "    train_pred_counts = predict_density_maps_and_get_counts(model, train_generator,\n",
    "                                                        config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    val_pred_counts = predict_density_maps_and_get_counts(model, val_generator,\n",
    "                                                          config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    test_pred_counts = predict_density_maps_and_get_counts(model, test_generator,\n",
    "                                                           config.DENSITY_MAP_MULTIPLICATION_FACTOR)\n",
    "    \n",
    "    ## 4. Predict and evaluate\n",
    "    train_results = evaluation_results_as_dict(train_gt_counts, train_pred_counts, 'train')\n",
    "    val_results = evaluation_results_as_dict(val_gt_counts, val_pred_counts, 'val')\n",
    "    test_results = evaluation_results_as_dict(test_gt_counts, test_pred_counts, 'test')\n",
    "\n",
    "    df = evaluation_results_as_df(train_results, val_results, test_results,\n",
    "                                  config.ARCHITECTURE_NAME,\n",
    "                                  config.SUB_EXPERIMENT_NAME,\n",
    "                                  config.DATASET_NAME)\n",
    "\n",
    "    df.to_csv(f'{QUANTITATIVE_RESULTS_PATH}/results.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
